{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0-dev\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import cv2\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import time\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "csv.field_size_limit(sys.maxsize);\n",
    "cur_dir = '/home/rob/Udacity/capstone/data'\n",
    "\n",
    "print cv2.__version__\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IM_ID = '6120_2_2'\n",
    "ClassNames = {'1':'buildings', '2':'Misc. Manmade structures', '3': 'Road', '4':'Track', '5':'Trees',\n",
    "                    '6':'Crops', '7':'Waterway', '8':'Standing water', '9':'Vehicle Large ', '10':'Vehicle Small'}\n",
    "\n",
    "save_masks = False\n",
    "\n",
    "\n",
    "def get_scalers(im_size):\n",
    "    h, w = im_size  # they are flipped so that mask_for_polygons works correctly\n",
    "    h, w = float(h), float(w)\n",
    "    w_ = w * (w / (w + 1))\n",
    "    h_ = h * (h / (h + 1))\n",
    "    return w_ / x_max, h_ / y_min\n",
    "\n",
    "def mask_for_polygons(polygons):\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "def scale_percentile(matrix):\n",
    "    w, h, d = matrix.shape\n",
    "    matrix = np.reshape(matrix, [w * h, d]).astype(np.float64)\n",
    "    # Get 2nd and 98th percentile\n",
    "    mins = np.percentile(matrix, 1, axis=0)\n",
    "    maxs = np.percentile(matrix, 99, axis=0) - mins\n",
    "    matrix = (matrix - mins[None, :]) / maxs[None, :]\n",
    "    matrix = np.reshape(matrix, [w, h, d])\n",
    "    matrix = matrix.clip(0, 1)\n",
    "    return matrix\n",
    "\n",
    "def show_mask(m):\n",
    "        \n",
    "    tiff.imshow(255 * np.stack([m, m, m]));\n",
    "    \n",
    "def mask_to_polygons(mask, epsilon=10., min_area=10.):\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    image, contours, hierarchy = cv2.findContours(\n",
    "        ((mask == 1) * 255).astype(np.uint8),\n",
    "        cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    # create approximate contours to have reasonable submission size\n",
    "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                       for cnt in contours]\n",
    "    if not contours:\n",
    "        return MultiPolygon()\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(approx_contours[idx])\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    for idx, cnt in enumerate(approx_contours):\n",
    "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
    "            assert cnt.shape[1] == 1\n",
    "            poly = Polygon(\n",
    "                shell=cnt[:, 0, :],\n",
    "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
    "                       if cv2.contourArea(c) >= min_area])\n",
    "            all_polygons.append(poly)\n",
    "    # approximating polygons might have created invalid ones, fix them\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n",
    "        # need to keep it a Multi throughout\n",
    "        if all_polygons.type == 'Polygon':\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "    return all_polygons\n",
    "\n",
    "def partial_pipe_fit(pipeline_obj, xs, ys):\n",
    "    xs = pipeline_obj.named_steps['standardscaler'].fit_transform(xs)\n",
    "    pipeline_obj.named_steps['sgdclassifier'].partial_fit(xs,ys,[0,1])\n",
    "\n",
    "\n",
    "class MY_SGDClassifier(SGDClassifier):\n",
    "    def __init__(self, n_jobs=3, n_iter=1, loss='log'):\n",
    "        super(MY_SGDClassifier, self).__init__(loss='log', penalty='l2', alpha=0.0001, \n",
    "                                               l1_ratio=0.15, fit_intercept=True, n_iter=n_iter, shuffle=True, \n",
    "                                               verbose=0, epsilon=0.1, n_jobs=n_jobs, random_state=None, \n",
    "                                               learning_rate='optimal', eta0=0.0, power_t=0.5, class_weight=None, \n",
    "                                               warm_start=False, average=False)\n",
    "    def fit(self, X, y, classes=[0,1], sample_weight=None):\n",
    "        self.partial_fit(X,y,classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "train_masks = {}\n",
    "pred_masks = {}\n",
    "\n",
    "trainIM_IDs = []\n",
    "with open(cur_dir + '/train_wkt_v4.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar=',')\n",
    "    for i,row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "        if (i%10) == 0:\n",
    "            trainIM_IDs.append(row[0])\n",
    "            \n",
    "testIM_IDs = []\n",
    "with open(cur_dir + '/sample_submission.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar=',')\n",
    "    for i,row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "        if (i%10) == 0:\n",
    "            testIM_IDs.append(row[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viewPictures = False\n",
    "\n",
    "def show_picture(IM_ID):\n",
    "    im_rgb = tiff.imread('../capstone/data/three_band/{}.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "    tiff.imshow(255 * scale_percentile(im_rgb[:,:]))\n",
    "\n",
    "if viewPictures:\n",
    "    for IM_ID in trainIM_IDs:\n",
    "        show_picture(IM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fitting the standardscaler to all data ###\n",
    "\n",
    "all_pictures = trainIM_IDs + testIM_IDs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for IM_ID in all_pictures:\n",
    "    \n",
    "    im_rgb = tiff.imread(cur_dir +'/three_band/{}.tif'.format(IM_ID)).transpose([1, 2, 0]) #gives back lists of lists\n",
    "    xs = im_rgb.reshape(-1, 3).astype(np.float32)\n",
    "    scaler.partial_fit(xs)\n",
    "\n",
    "mean = scaler.mean_\n",
    "std = scaler.var_\n",
    "scale = scaler.scale_\n",
    "samples_seen = scaler.n_samples_seen_\n",
    "a = scaler.get_params()\n",
    "\n",
    "print mean, std, scale, samples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Training SGD on Data\n",
    "\n",
    "\n",
    "counter = 1\n",
    "train_set = trainIM_IDs[1:] + [trainIM_IDs[0]]\n",
    "#train_set = trainIM_IDs[1:2] + trainIM_IDs[3:12] + trainIM_IDs[13:16] + trainIM_IDs[17:19] + trainIM_IDs[20:]\n",
    "test_set = [trainIM_IDs[0],trainIM_IDs[2],trainIM_IDs[12], trainIM_IDs[16], trainIM_IDs[19]]\n",
    "\n",
    "trainDataLength = len(train_set)\n",
    "\n",
    "(l1,l2,l3,l4,l5) = [],[],[],[],[]\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "\n",
    "for IM_ID in train_set:\n",
    "    print IM_ID\n",
    "    print \"{}. picture of {}\".format(counter, trainDataLength)\n",
    "    \n",
    "    x_max = y_min = None\n",
    "    for _im_id, _x, _y in csv.reader(open(cur_dir + '/grid_sizes.csv')):\n",
    "        if _im_id == IM_ID:\n",
    "            x_max, y_min = float(_x), float(_y)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    for i in range(1,11): #range(1,11)\n",
    "        POLY_TYPE = str(i)\n",
    "        print ClassNames[POLY_TYPE]\n",
    "        l1.append(IM_ID)\n",
    "        l2.append(ClassNames[POLY_TYPE])\n",
    "\n",
    "        # Load train poly with shapely\n",
    "        train_polygons = None\n",
    "        for _im_id, _poly_type, _poly in csv.reader(open(cur_dir + '/train_wkt_v4.csv')):\n",
    "            if _im_id == IM_ID and _poly_type == POLY_TYPE:\n",
    "                train_polygons = shapely.wkt.loads(_poly)\n",
    "                break\n",
    "                \n",
    "        im_rgb = tiff.imread(cur_dir +'/three_band/{}.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "        im_size = im_rgb.shape[:2]\n",
    "        \n",
    "        \"\"\"\n",
    "        # Read image with tiff\n",
    "        im_rgb = tiff.imread(cur_dir +'/sixteen_band/{}_M.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "        im_size = im_rgb.shape[:2]\n",
    "        img = np.zeros((im_rgb.shape[0],im_rgb.shape[1],3))\n",
    "        img[:,:,0] = im_rgb[:,:,7] #red\n",
    "        img[:,:,1] = im_rgb[:,:,4] #green\n",
    "        img[:,:,2] = im_rgb[:,:,0] #blue\n",
    "        im_rgb = img\n",
    "        \"\"\"\n",
    "        x_scaler, y_scaler = get_scalers(im_size)\n",
    "\n",
    "        train_polygons_scaled = shapely.affinity.scale(\n",
    "            train_polygons, xfact=x_scaler, yfact=y_scaler, origin=(0, 0, 0))\n",
    "\n",
    "        train_mask = mask_for_polygons(train_polygons_scaled)\n",
    "        \n",
    "        xs = im_rgb.reshape(-1, 3).astype(np.float32)\n",
    "        ys = train_mask.reshape(-1)\n",
    "        xs = scaler.transform(xs)\n",
    "        \n",
    "        \"\"\"Initialize pipeline\"\"\"\n",
    "        if IM_ID == train_set[0]:\n",
    "            #pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss='log',n_iter=1))\n",
    "            pipe2 = MY_SGDClassifier(loss='log',n_iter=1)\n",
    "        else:\n",
    "            pipe2 = classifiers[\"SGD{}\".format(ClassNames[POLY_TYPE])]\n",
    "\n",
    "        print('training...')\n",
    "        \n",
    "        #partial_pipe_fit(pipeline, xs, ys)\n",
    "        #pred_ys = pipeline.predict_proba(xs)[:, 1]\n",
    "        \n",
    "        try:\n",
    "            pipe2 = pipe2.partial_fit(xs,ys, [0,1])\n",
    "            pred_ys = (pipe2.predict_proba(xs).T)[1]\n",
    "        except ValueError:\n",
    "            \"Class not represented\"\n",
    "            classifiers[\"SGD{}\".format(ClassNames[POLY_TYPE])] = pipe2\n",
    "            continue\n",
    "        pred_mask = pred_ys.reshape(train_mask.shape)\n",
    "        threshold = 0.30\n",
    "        pred_binary_mask = pred_mask >= threshold\n",
    "\n",
    "        #Save all for later use\n",
    "        if save_masks:\n",
    "            pred_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])] = pred_mask\n",
    "            train_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])] = train_mask\n",
    "\n",
    "        classifiers[\"SGD{}\".format(ClassNames[POLY_TYPE])] = pipe2\n",
    "\n",
    "        pred_polygons = mask_to_polygons(pred_binary_mask,epsilon=10., min_area=10.)\n",
    "        pred_poly_mask = mask_for_polygons(pred_polygons)\n",
    "\n",
    "        scaled_pred_polygons = shapely.affinity.scale(\n",
    "            pred_polygons, xfact=1 / x_scaler, yfact=1 / y_scaler, origin=(0, 0, 0))\n",
    "\n",
    "        dumped_prediction = shapely.wkt.dumps(scaled_pred_polygons) #This is exactly the wanted output for submission\n",
    "        final_polygons = shapely.wkt.loads(dumped_prediction)\n",
    "\n",
    "        if final_polygons == \"GEOMETRYCOLLECTION EMPTY\":\n",
    "            print \"Algorithm did not detect this class or it is not in dataset\"\n",
    "        else:\n",
    "            tp, fp, fn = (( pred_binary_mask &  train_mask).sum(),\n",
    "                      ( pred_binary_mask & ~train_mask).sum(),\n",
    "                      (~pred_binary_mask &  train_mask).sum())\n",
    "            avg_prec_score = average_precision_score(ys, pred_ys)\n",
    "            avg_recall = recall_score(ys, pred_binary_mask.reshape(-1))\n",
    "            print('average precision (correct prediction(tp)/ all predicions, true or false (tp+fp))', avg_prec_score)\n",
    "            print('recall_score (correct prediction(tp)/ real amount of that class(tp+fn)) ', avg_recall)\n",
    "            pixel_jaccard = (float(tp) / (tp + fp + fn))\n",
    "            print('Pixel jaccard', pixel_jaccard)\n",
    "            print('Prediction size: {:,} bytes'.format(len(dumped_prediction)))\n",
    "            l3.append(avg_recall)\n",
    "            l4.append(pixel_jaccard)\n",
    "            try:\n",
    "                final_jaccard = (final_polygons.intersection(train_polygons).area /\n",
    "                      final_polygons.union(train_polygons).area)\n",
    "                print('Final jaccard',final_jaccard)\n",
    "                l5.append(final_jaccard)\n",
    "            except ZeroDivisionError:\n",
    "                print \"Warning: Error in Final jaccard calculation!\"\n",
    "        print \"\"\n",
    "        print \"\"\n",
    "        \n",
    "        \n",
    "    counter += 1\n",
    "timer_end = time.time()\n",
    "print (timer_end - timer)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records = pd.DataFrame([l1,l2,l3,l4,l5],index=['IM_ID','CLASS','PRECI','PIXEL JAC','FINAL JAC']).T\n",
    "print records.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###  --- Test the train result on validation data. This is not used right now --- ###\n",
    "\n",
    "if False:\n",
    "    for IM_ID in test_set:\n",
    "        print IM_ID\n",
    "        print \"{}. picture\".format(counter)\n",
    "\n",
    "        x_max = y_min = None\n",
    "        for _im_id, _x, _y in csv.reader(open(cur_dir + '/grid_sizes.csv')):\n",
    "            if _im_id == IM_ID:\n",
    "                x_max, y_min = float(_x), float(_y)\n",
    "                break\n",
    "\n",
    "\n",
    "        for i in range(1,11): #range(1,11)\n",
    "            POLY_TYPE = str(i)\n",
    "            print ClassNames[POLY_TYPE]\n",
    "            l1.append(IM_ID)\n",
    "            l2.append(ClassNames[POLY_TYPE])\n",
    "\n",
    "            im_rgb = tiff.imread(cur_dir +'/three_band/{}.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "            im_size = im_rgb.shape[:2]\n",
    "            \"\"\"\n",
    "            # Read image with tiff\n",
    "            im_rgb = tiff.imread(cur_dir +'/sixteen_band/{}_M.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "            im_size = im_rgb.shape[:2]\n",
    "            img = np.zeros((im_rgb.shape[0],im_rgb.shape[1],3))\n",
    "            img[:,:,0] = im_rgb[:,:,7] #red\n",
    "            img[:,:,1] = im_rgb[:,:,4] #green\n",
    "            img[:,:,2] = im_rgb[:,:,0] #blue\n",
    "            im_rgb = img\n",
    "            \"\"\"\n",
    "\n",
    "            x_scaler, y_scaler = get_scalers()\n",
    "\n",
    "            train_polygons_scaled = shapely.affinity.scale(\n",
    "                train_polygons, xfact=x_scaler, yfact=y_scaler, origin=(0, 0, 0))\n",
    "\n",
    "\n",
    "            train_mask = mask_for_polygons(train_polygons_scaled)\n",
    "\n",
    "            xs = im_rgb.reshape(-1, 3).astype(np.float32)\n",
    "            ys = train_mask.reshape(-1)\n",
    "\n",
    "\n",
    "            \"\"\"Benutze train pipeline\"\"\"\n",
    "\n",
    "            pipeline = classifiers[\"SGD{}\".format(ClassNames[POLY_TYPE])]\n",
    "\n",
    "            print('testing...')\n",
    "\n",
    "            pred_ys = pipeline.predict_proba(xs)[:, 1]\n",
    "            pred_mask = pred_ys.reshape(train_mask.shape)\n",
    "\n",
    "            threshold = 0.31\n",
    "            pred_binary_mask = pred_mask >= threshold\n",
    "\n",
    "            #Save all for later use\n",
    "            if save_masks:\n",
    "                pred_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])] = pred_mask\n",
    "                train_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])] = train_mask\n",
    "\n",
    "            pred_polygons = mask_to_polygons(pred_binary_mask)\n",
    "            pred_poly_mask = mask_for_polygons(pred_polygons)\n",
    "\n",
    "            scaled_pred_polygons = shapely.affinity.scale(\n",
    "                pred_polygons, xfact=1 / x_scaler, yfact=1 / y_scaler, origin=(0, 0, 0))\n",
    "\n",
    "            dumped_prediction = shapely.wkt.dumps(scaled_pred_polygons) #This is exactly the wanted output for submission\n",
    "            final_polygons = shapely.wkt.loads(dumped_prediction)\n",
    "\n",
    "            if dumped_prediction == \"GEOMETRYCOLLECTION EMPTY\":\n",
    "                print \"No pixels predicted by algorithm\"\n",
    "            else:\n",
    "                tp, fp, fn = (( pred_binary_mask &  train_mask).sum(),\n",
    "                          ( pred_binary_mask & ~train_mask).sum(),\n",
    "                          (~pred_binary_mask &  train_mask).sum())\n",
    "                print('average precision', average_precision_score(ys, pred_ys))\n",
    "                pixel_jaccard = (float(tp) / (tp + fp + fn))\n",
    "                print('Pixel jaccard', pixel_jaccard)\n",
    "                print('Prediction size: {:,} bytes'.format(len(dumped_prediction)))\n",
    "                l3.append(average_precision_score)\n",
    "                l4.append(pixel_jaccard)\n",
    "                try:\n",
    "                    final_jaccard = (final_polygons.intersection(train_polygons).area /\n",
    "                          final_polygons.union(train_polygons).area)\n",
    "                    print('Final jaccard',final_jaccard)\n",
    "                    l5.append(final_jaccard)\n",
    "                except TopologicalError:\n",
    "                    print \"Warning: Error in Final jaccard calculation!\"\n",
    "            print \"\"\n",
    "            print \"\"\n",
    "\n",
    "    pd.DataFrame([l1,l2,l3,l4,l5],index=['IM_ID','CLASS','PRECI','PIXEL JAC','FINAL JAC']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if save_masks:\n",
    "    for i in range(1,11):\n",
    "        POLY_TYPE = str(i)\n",
    "        show_mask(pred_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])][:,:])\n",
    "        show_mask(train_masks[\"SGD{}\".format(ClassNames[POLY_TYPE])][:,:])\n",
    "        #show_mask(pred_mask[:,:])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get picture ID\n",
    "\n",
    "train_polygons = None\n",
    "\n",
    "big_timer = time.time()\n",
    "predictions = []\n",
    "picture_counter = 1\n",
    "for IM_ID in testIM_IDs:\n",
    "    print IM_ID\n",
    "    print \"{}. picture out of {}\".format(picture_counter, len(testIM_IDs))\n",
    "    picture_counter += 1\n",
    "    start = time.time()\n",
    "    \n",
    "    x_max = y_min = None\n",
    "    for _im_id, _x, _y in csv.reader(open(cur_dir + '/grid_sizes.csv')):\n",
    "        if _im_id == IM_ID:\n",
    "            x_max, y_min = float(_x), float(_y)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    for i in range(1,11):\n",
    "        POLY_TYPE = str(i)\n",
    "\n",
    "        # Read image with tiff\n",
    "        im_rgb = tiff.imread(cur_dir +'/three_band/{}.tif'.format(IM_ID)).transpose([1, 2, 0])\n",
    "        im_size = im_rgb.shape[:2]\n",
    "\n",
    "        x_scaler, y_scaler = get_scalers(im_size)\n",
    "\n",
    "        xs = im_rgb.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "        pipe2 = classifiers[\"SGD{}\".format(ClassNames[POLY_TYPE])]\n",
    "        xs = scaler.transform(xs)\n",
    "        try:\n",
    "            pred_ys = pipe2.predict(xs) #pipeline.predict_proba(xs)[:, 1]\n",
    "        except IndexError:\n",
    "            print (\"No prediction possible. Instead prediction mask will\"\n",
    "                   \"be zeros only\")\n",
    "            pred_ys = np.zeros(im_size)\n",
    "\n",
    "        pred_mask = pred_ys.reshape(im_size)\n",
    "\n",
    "        threshold = 0.30\n",
    "        pred_binary_mask = pred_mask >= threshold\n",
    "        pred_polygons = mask_to_polygons(pred_binary_mask, epsilon=10., min_area=10.)\n",
    "        scaled_pred_polygons = shapely.affinity.scale(\n",
    "            pred_polygons, xfact=1 / x_scaler, yfact=1 / y_scaler, origin=(0, 0, 0))\n",
    "\n",
    "        dumped_prediction = shapely.wkt.dumps(scaled_pred_polygons)\n",
    "        if dumped_prediction == 'GEOMETRYCOLLECTION EMPTY':\n",
    "            dumped_prediction = 'MULTIPOLYGON EMPTY'\n",
    "\n",
    "        predictions.append([IM_ID,POLY_TYPE, dumped_prediction])\n",
    "        \n",
    "    end = time.time()\n",
    "    print \"Prediction took\",(end-start), \"seconds\"\n",
    "    \n",
    "big_timer_end = time.time()\n",
    "print \"Prediction time for all pictures in Minutes: \", (big_timer_end- big_timer)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"predictions6.csv\",\"wb\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['ImageId','ClassType','MultipolygonWKT'])\n",
    "    for prediction in predictions:\n",
    "        writer.writerow(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
